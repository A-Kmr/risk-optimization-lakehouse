{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "512a3926-c370-481c-b9c9-957d5e0531cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 03_Train_PD_Model (Final Robust Version)\n",
    "# Goal: Train a Gradient Boosted Tree (Handling Dirty Data)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# 1. Load Silver Data\n",
    "print(\"‚è≥ Loading Silver Table...\")\n",
    "df = spark.table(\"silver_lending_club\")\n",
    "\n",
    "# --- THE FIX: Use 'try_cast' instead of 'cast' ---\n",
    "# try_cast returns NULL for bad data (like \"Debt consolidation\") instead of crashing.\n",
    "print(\"üîß Fixing Data Types & Dropping Bad Rows...\")\n",
    "\n",
    "# Note: We use expr(\"try_cast(col as type)\") style or the try_cast function if available\n",
    "# The safest way compatible with all versions is using selectExpr or withColumn with explicit casting logic\n",
    "df_clean = df.select(\n",
    "    col(\"default_flag\"),\n",
    "    col(\"home_ownership\"),\n",
    "    col(\"purpose\"),\n",
    "    col(\"addr_state\"),\n",
    "    col(\"term_clean\"),\n",
    "    col(\"emp_length_clean\"),\n",
    "    col(\"annual_inc\").try_cast(\"float\").alias(\"annual_inc\"),\n",
    "    col(\"dti\").try_cast(\"float\").alias(\"dti\"),\n",
    "    col(\"loan_amnt\").try_cast(\"float\").alias(\"loan_amnt\"),\n",
    "    col(\"int_rate\").try_cast(\"float\").alias(\"int_rate\"),\n",
    "    col(\"installment\").try_cast(\"float\").alias(\"installment\")\n",
    ").dropna() \n",
    "\n",
    "# 2. Split Data (Train / Test)\n",
    "print(\"‚úÇÔ∏è Splitting Data (80% Train, 20% Test)...\")\n",
    "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"   Train Rows: {train_data.count():,}\")\n",
    "print(f\"   Test Rows:  {test_data.count():,}\")\n",
    "\n",
    "# 3. Define Features\n",
    "cat_cols = [\"home_ownership\", \"purpose\", \"addr_state\"]\n",
    "num_cols = [\"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\", \n",
    "            \"dti\", \"term_clean\", \"emp_length_clean\"]\n",
    "\n",
    "# 4. Build the ML Pipeline\n",
    "# Step A: Convert Strings to Numbers (StringIndexer)\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid=\"keep\") \n",
    "    for c in cat_cols\n",
    "]\n",
    "\n",
    "# Step B: Assemble all features into one vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_index\" for c in cat_cols] + num_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\" \n",
    ")\n",
    "\n",
    "# Step C: The Model (Gradient Boosted Tree)\n",
    "# --- THE FIX: Set maxBins to 64 so it can handle all 50 US States ---\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"default_flag\", \n",
    "    featuresCol=\"features\", \n",
    "    maxIter=20,\n",
    "    maxBins=64  # <--- THIS IS THE FIX (Default is 32)\n",
    ")\n",
    "\n",
    "# Step D: Chain it all together\n",
    "pipeline = Pipeline(stages=indexers + [assembler, gbt])\n",
    "\n",
    "# 5. Train the Model\n",
    "print(\"üöÇ Training Gradient Boosted Tree (This takes 2-5 mins)...\")\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# 6. Make Predictions\n",
    "print(\"üîÆ Generating Predictions...\")\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# 7. Evaluate Performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"default_flag\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"üöÄ MODEL TRAINED!\")\n",
    "print(f\"üéØ Test AUC Score: {auc:.3f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 8. Save the Model\n",
    "model_path = \"/Volumes/workspace/default/raw_data/gbt_credit_model\"\n",
    "print(f\"üíæ Saving model to {model_path}...\")\n",
    "model.write().overwrite().save(model_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Train_PD_Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
